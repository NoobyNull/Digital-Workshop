[tool:pytest]
# Pytest configuration for Unified Test Execution System
# This configuration supports parallel execution, comprehensive coverage reporting,
# and intelligent test discovery across the entire codebase.

minversion = 6.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=src
    --cov-report=term-missing
    --cov-report=html:reports/coverage_html
    --cov-report=xml:reports/coverage.xml
    --cov-report=json:reports/coverage.json
    --junit-xml=reports/junit.xml
    --html=reports/pytest_report.html
    --self-contained-html
    --json-report
    --json-report-file=reports/pytest_report.json
    --durations=10
    --timeout=300
    --log-cli=true
    --log-cli-level=INFO
    --log-cli-format=%(asctime)s [%(levelname)8s] %(name)s: %(message)s
    --log-cli-date-format=%Y-%m-%d %H:%M:%S

# Test discovery paths
testpaths = tests src

# Test file patterns
python_files = test_*.py *_test.py

# Test class patterns
python_classes = Test*

# Test function patterns
python_functions = test_*

# Test markers for categorization and filtering
markers =
    unit: Unit tests for individual components
    integration: Integration tests for component interactions
    performance: Performance and load tests
    e2e: End-to-end workflow tests
    quality: Code quality and linting tests
    slow: Tests that take longer than 5 seconds
    fast: Tests that complete quickly
    network: Tests that require network access
    database: Tests that require database access
    gui: Tests that require GUI components
    parser: Tests for file parsers
    model: Tests for 3D model processing
    workflow: Tests for complete workflows
    regression: Regression tests for bug fixes
    smoke: Smoke tests for basic functionality
    critical: Critical tests that must pass
    optional: Optional tests that can fail without blocking

# Warning filters
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::FutureWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    error::pytest.PytestDeprecationWarning

# Timeout settings
timeout = 300
timeout_method = thread

# Logging configuration
log_file = reports/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Parallel execution settings (for pytest-xdist)
# Use -n auto for automatic CPU core detection
# Use -n logical for logical CPU count
# Use -n <number> for specific worker count

# Coverage settings
[coverage:run]
source = src
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */virtualenv/*
    */.tox/*
    */site-packages/*
    setup.py
    conftest.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[coverage:html]
directory = reports/coverage_html

[coverage:xml]
output = reports/coverage.xml

[coverage:json]
output = reports/coverage.json

# JMeter XML output for CI/CD integration
junit_family = xunit2
junit_logging = all

# Test collection settings
collect_ignore =
    setup.py
    conftest.py
    build.py
    config/
    scripts/
    docs/
    resources/
    tools/
    archive/
    .specify/
    specs/
    reports/
    test_results/
    shutdown_analysis_reports/

# Parallel execution hints
# To use parallel execution, install pytest-xdist: pip install pytest-xdist
# Then run with: pytest -n auto
# Or specify worker count: pytest -n 4

# Performance testing configuration
# For performance tests, use: pytest -m performance --benchmark-only
# Requires: pip install pytest-benchmark

# Memory leak testing configuration  
# For memory testing, use: pytest -m "not slow" --memray
# Requires: pip install memray

# Test output formatting
console_output_style = progress

# Cache settings
cache_dir = .pytest_cache

# Plugin settings
[pytest-html]
# HTML report settings
render_collapsed = True

[pytest-json-report]
# JSON report settings
metadata = 
    Platform: %(platform)s
    Python: %(python)s
    Pytest: %(pytest)s

[pytest-cov]
# Coverage settings
cov_fail_under = 80
show_missing = True
skip_covered = False

[pytest-timeout]
# Timeout settings
timeout = 300
method = thread

[pytest-xdist]
# Parallel execution settings
# Uncomment to enable automatic parallel execution
# addopts = -n auto

# Worker settings
# max_workers = auto
# worker_class = thread
# worker_loop = scope

# Test distribution
# Distribute tests evenly across workers
# This is enabled by default with -n auto

# Load balancing
# Tests are distributed based on duration estimates
# to balance workload across workers